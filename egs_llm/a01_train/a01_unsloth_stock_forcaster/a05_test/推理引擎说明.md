# 简介

**资源**
- Unsloth官方文档：https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune
- Jupyter: https://colab.research.google.com/drive/1aqlNQi7MMJbynFDyOQteD2t0yVfjb9Zh?usp=sharing#scrollTo=Yvwf0OEtchS_

**各类推理方案对比**
- Unsloth自带推理引擎：
  - 速度快，但没有OpenAI接口
  - 初始化启动很慢，约1-2分钟
- vLLM
  - 可能需自己编译，最新的预编译版只有CUDA12.6的
- Ollama
  - Unsloth给出的介绍有些简单，特别你要是使用本地模型，会遇到更多问题，请参见ollama的文档
  - 配置比vLLM复杂
- transformers
  - 启动速度快，但没有OpenAI接口
  
官方推荐推理参数：

| **Non-Thinking Mode Settings:**                                 | **Thinking Mode Settings:** |
|------------------------------------------------------------------|------------------------------|
| **Temperature = 0.7**                                            | **Temperature = 0.6**        |
| Min_P = 0.0 (optional, but 0.01 works well, llama.cpp default is 0.1) | Min_P = 0.0                  |
| Top_P = 0.8                                                      | Top_P = 0.95                 |
| TopK = 20                                                        | TopK = 20                    |


# 1. vLLM
如果使用vLLM，用如下命令安装（注意，最新预编译版只有CUDA12.6的，如果需要低于这个，得自己源码编译）：

>pip install vllm

# 2. Ollama

- Qwen3更多的使用安装方法见(235B-A22B版本部署方式不一样)：https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune#ollama-run-qwen3-tutorial
- 如果下面方式无法启动，查看更详细的说明`ollama使用02`

**1.安装**
```bash
apt-get update
apt-get install pciutils -y
curl -fsSL https://ollama.com/install.sh | sh
```

默认启动地址：127.0.0.1:11434

**2.运行**

如果无法启动，则新开一个窗口，或者查阅`ollama使用说明`

```bash
ollama run hf.co/unsloth/Qwen3-8B-GGUF:Q4_K_XL
```

**3.问答**
如果不思考，可以改system提示词，或者直接使用下面的命令：
```bash
>>> Write your prompt here /nothink
```
